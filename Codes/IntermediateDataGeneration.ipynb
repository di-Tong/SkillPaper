{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ditong\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Word2VecVocab\n",
    "import math \n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import permutation\n",
    "import operator\n",
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import pylab\n",
    "from scipy.stats import pearsonr \n",
    "import seaborn as sns\n",
    "import sklearn.metrics.pairwise\n",
    "from statistics import mean, stdev\n",
    "import statsmodels.api as sm\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from zipfile import ZipFile \n",
    "from ksvd import ApproximateKSVD #pip or conda install ksvd #this is key!\n",
    "from __future__ import division\n",
    "import cython\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from itertools import combinations\n",
    "import random\n",
    "import pywren\n",
    "\n",
    "def norm(a):\n",
    "    row_sums = a.sum(axis=1)\n",
    "    new_matrix = a / row_sums[:, np.newaxis]\n",
    "    return new_matrix\n",
    "\n",
    "def flushPrint(d): # counter\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(str(d))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. year, socName, N posts, ave. pay, ave. edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract year, socName, N posts, ave. pay, ave. edu from raw main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "for i in range(2010, 2020):\n",
    "    year = str(i)\n",
    "    folder = '/project2/jevans/BG/Structured_Data/Main/' + year + '/'\n",
    "    for f in listdir(folder):\n",
    "        if not f.startswith('.'):\n",
    "            print(f)\n",
    "#             try:\n",
    "#             # unzip and read each file into dataframe\n",
    "#                 with ZipFile(folder+f) as z:\n",
    "#                     with z.open(f.split('.')[0]+'.txt') as f1:\n",
    "#                         df = pd.read_csv(f1,header='infer', delimiter=\"\\t\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "#             except:\n",
    "#                 # dealing with the broken file separately\n",
    "#                 df = pd.read_csv('/project2/jevans/ditong/BG/Main_2011-04.txt', \n",
    "#                      delimiter=\"\\t\", header='infer',encoding = \"ISO-8859-1\", low_memory=False) \n",
    "            with ZipFile(folder+f) as z:\n",
    "                with z.open(f.split('.')[0]+'.txt') as f1:\n",
    "                    df = pd.read_csv(f1,header='infer',delimiter=\"\\t\",encoding = \"ISO-8859-1\", low_memory=False)\n",
    "                    for a,b,c,d,e,f,g,h,i,j,k in zip(df['BGTJobId'],df['JobDate'],df['CleanTitle'],df['SOCName'],\n",
    "                                    df['MinSalary'], df['MaxSalary'], df['Edu'], df['MaxEdu'],\n",
    "                                    df['JobHours'], df['TaxTerm'], df['Internship']): \n",
    "                        line1 = \"\\t\".join([str(a), str(c)])\n",
    "                        line2 = \"\\t\".join([str(a), str(b), str(d), str(e)+'_'+str(f), str(g)+'_'+str(h), \n",
    "                                           str(i), str(j), str(k)])\n",
    "                        with open('/project2/jevans/ditong/data/SOC/jobid_date_soc_pay_edu_fullPartTime_employeeContractor_internOrNot.txt', \n",
    "                                  'w') as f2:\n",
    "                            f2.write(line2 + \"\\n\")\n",
    "                    \n",
    "elapsed = (time.clock() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce year- socName, Nposts, pay list, education dictionaries; year-socName-jobPostId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187.0"
     ]
    }
   ],
   "source": [
    "#start = time.clock()\n",
    "yjn = defaultdict(lambda:defaultdict(lambda:0))\n",
    "yjpl = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "yjel = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "yjmaxel = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "yjmedel = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "n=0\n",
    "dic_job_id = defaultdict(lambda:set())\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/jobid_date_soc_pay_edu_fullPartTime_employeeContractor_internOrNot.txt',\n",
    "          'r') as f:\n",
    "    for line in f:\n",
    "        n+=1\n",
    "        if n>1000000 and n % 1000000 == 0:\n",
    "            flushPrint(n/1000000)\n",
    "        line_lst = line.split('\\t')\n",
    "        jobid = line_lst[0]\n",
    "        year = int(line_lst[1][:4])\n",
    "        soc = line_lst[2]\n",
    "        pay = line_lst[3]\n",
    "        edu = line_lst[4]\n",
    "        #jobhours = line_lst[5]\n",
    "        #taxterm = line_lst[6]  \n",
    "        #intern = int(line_lst[7])\n",
    "        if soc not in ['na', 'nan']:\n",
    "            # count occupation job post number\n",
    "            yjn[year][soc] += 1\n",
    "            \n",
    "            # list of median pay\n",
    "            minp, maxp=[float(num) for num in pay.split(\"_\")]\n",
    "            if minp >0 and maxp > 0:\n",
    "                medp = minp + (maxp - minp) / 2\n",
    "                yjpl[year][soc].append(medp)\n",
    "            \n",
    "            # list of min education\n",
    "            mine, maxe=[float(num) for num in edu.split(\"_\")]\n",
    "            if mine >0:\n",
    "                yjel[year][soc].append(mine)\n",
    "            if maxe >0:\n",
    "                yjmaxel[year][soc].append(maxe)\n",
    "                if mine > 0:\n",
    "                    mede = mine + (maxe - mine) / 2\n",
    "                    yjmedel[year][soc].append(mede)\n",
    "                \n",
    "            # socName-jobid\n",
    "            dic_job_id[soc].add(jobid)\n",
    "            \n",
    "#elapsed = (time.clock() - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yje = defaultdict(lambda:defaultdict(lambda:0))\n",
    "yjmaxe = defaultdict(lambda:defaultdict(lambda:0))\n",
    "yjmede = defaultdict(lambda:defaultdict(lambda:0))\n",
    "yjp = defaultdict(lambda:defaultdict(lambda:0))\n",
    "jye = defaultdict(lambda:defaultdict(lambda:0))\n",
    "jyp = defaultdict(lambda:defaultdict(lambda:0))\n",
    "for year in range(2010, 2020):\n",
    "    for job in yjel[year]:\n",
    "        yje[year][job] = mean(yjel[year][job])\n",
    "        jye[job][year] = yje[year][job]\n",
    "    for job in yjmaxel[year]:\n",
    "        yjmaxe[year][job] = mean(yjmaxel[year][job])\n",
    "        #jye[job][year] = yje[year][job]\n",
    "    for job in yjmedel[year]:\n",
    "        yjmede[year][job] = mean(yjmedel[year][job])\n",
    "        #jye[job][year] = yje[year][job]\n",
    "    for job in yjpl[year]:\n",
    "        yjp[year][job] = mean(yjpl[year][job])\n",
    "        jyp[job][year] = yjp[year][job]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yjn), len(yje), len(yjp), len(yjmaxe), len(yjmede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min education, max education, median education\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/year_socName_aveMinEdu_avgMaxEdu_avgMedEdu.txt',\n",
    "          'w') as f:\n",
    "    for year in yje:\n",
    "        for job in yje[year]:\n",
    "            if job in yjmaxe[year]:\n",
    "                if job in yjmede[year]:\n",
    "                    line = \"\\t\".join([str(year), job, str(yje[year][job]), str(yjmaxe[year][job]), str(yjmede[year][job])])\n",
    "                else:\n",
    "                    line = \"\\t\".join([str(year), job, str(yje[year][job]), str(yjmaxe[year][job]), 'nan'])\n",
    "            else:\n",
    "                if job in yjmede[year]:\n",
    "                    line = \"\\t\".join([str(year), job, str(yje[year][job]), 'nan', str(yjmede[year][job])])\n",
    "                else:\n",
    "                    line = \"\\t\".join([str(year), job, str(yje[year][job]), 'nan', 'nan'])\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/year_socName_Nposts_aveMedianPay_aveMinEdu.txt',\n",
    "          'w') as f:\n",
    "    for year in yjn:\n",
    "        for job in yjn[year]:\n",
    "            if job in yje[year]:\n",
    "                if job in yjp[year]:\n",
    "                    line = \"\\t\".join([str(year), job, str(yjn[year][job]), str(yje[year][job]), str(yjp[year][job])])\n",
    "                else:\n",
    "                    line = \"\\t\".join([str(year), job, str(yjn[year][job]), str(yje[year][job]), 'nan'])\n",
    "            else:\n",
    "                if job in yjp[year]:\n",
    "                    line = \"\\t\".join([str(year), job, str(yjn[year][job]), 'nan', str(yjp[year][job])])\n",
    "                else:\n",
    "                    line = \"\\t\".join([str(year), job, str(yjn[year][job]), 'nan', 'nan'])\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project2/jevans/ditong/data/SOC/' + year + '/' + year + '_job_id.txt', 'w') as f:\n",
    "    for job in dic_job_id:\n",
    "        ids = \"_\".join(list(dic_job_id[job]))\n",
    "        line = \"\\t\".join([job, ids])\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. job - meta (ave. pay, ave. edu, 2-digits soc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in BLS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLS 2010, 2018 Occupational employment volume, 2018 pay\n",
    "df_2018=pd.read_excel('/Users/ditong/Dropbox (MIT)/skills/Data/raw/national_M2018_dl.xlsx')\n",
    "ocg={}\n",
    "cg = {}\n",
    "for i, j, k in zip(df_2018['OCC_TITLE'], df_2018['OCC_GROUP'], df_2018['OCC_CODE']):\n",
    "    if j=='major':\n",
    "        cg[k[:2]] = i\n",
    "    elif j == 'detailed':\n",
    "        ocg[i] = cg[k[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocg['Adult Basic and Secondary Education and Literacy Teachers and Instructors '] = 'Education, Training, and Library Occupations'\n",
    "ocg['Assemblers and Fabricators, All Other'] = 'Production Occupations'\n",
    "ocg['Bus Drivers, School or Special Client '] = 'Transportation and Material Moving Occupations'\n",
    "ocg['Bus Drivers, Transit and Intercity '] = 'Transportation and Material Moving Occupations'\n",
    "ocg['Buyers and Purchasing Agents, Farm Products'] = 'Business and Financial Operations Occupations'\n",
    "ocg['Construction and Related Workers, All Other'] = 'Construction and Extraction Occupations'\n",
    "ocg['Electrical and Electronic Equipment Assemblers'] = 'Production Occupations'\n",
    "ocg['Electromechanical Equipment Assemblers'] = 'Production Occupations'\n",
    "ocg['First-Line Supervisors of Helpers, Laborers, and Material Movers, Hand'] = 'Transportation and Material Moving Occupations'\n",
    "ocg['First-Line Supervisors of Transportation and Material-Moving Machine and Vehicle Operators'] = 'Transportation and Material Moving Occupations'\n",
    "ocg['Fishers and Related Fishing Workers'] = 'Farming, Fishing, and Forestry Occupations'\n",
    "ocg['Gaming Supervisors'] = 'Personal Care and Service Occupations'\n",
    "ocg['Health Educators '] = 'Community and Social Service Occupations'\n",
    "ocg['Hunters and Trappers'] = 'Farming, Fishing, and Forestry Occupations'\n",
    "ocg['Mathematical Science Occupations, All Other'] = 'Computer and Mathematical Occupations'\n",
    "ocg['Medical and Clinical Laboratory Technicians'] = 'Healthcare Practitioners and Technical Occupations'\n",
    "ocg['Medical and Clinical Laboratory Technologists'] = 'Healthcare Practitioners and Technical Occupations'\n",
    "ocg['Mental Health Counselors'] = 'Community and Social Service Occupations'\n",
    "ocg['Purchasing Agents, Except Wholesale, Retail, and Farm Products'] = 'Business and Financial Operations Occupations'\n",
    "ocg['Radio, Cellular, and Tower Equipment Installers and Repairs'] = 'Installation, Maintenance, and Repair Occupations'\n",
    "ocg['Radiologic Technologists '] = 'Healthcare Practitioners and Technical Occupations'\n",
    "ocg['Segmental Pavers'] = 'Construction and Extraction Occupations'\n",
    "ocg['Slot Supervisors'] = 'Personal Care and Service Occupations'\n",
    "ocg['Substance Abuse and Behavioral Disorder Counselors'] = 'Community and Social Service Occupations'\n",
    "ocg['Teachers and Instructors, All Other'] = 'Education, Training, and Library Occupations'\n",
    "ocg['Team Assemblers'] = 'Production Occupations'\n",
    "ocg['Tour Guides and Escorts'] = 'Personal Care and Service Occupations'\n",
    "ocg['Transportation Attendants, Except Flight Attendants '] = 'Transportation and Material Moving Occupations'\n",
    "ocg['Travel Guides'] = 'Personal Care and Service Occupations'\n",
    "ocg['Wholesale and Retail Buyers, Except Farm Products'] = 'Business and Financial Operations Occupations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annual average education level for each occupation\n",
    "oe = {}\n",
    "for job in jye:\n",
    "    oe[job] = np.mean([jye[job][yr] for yr in jye[job] if yr != 2019])\n",
    "# get annual average pay level for each occupation\n",
    "op = {}\n",
    "for job in jyp:\n",
    "    op[job] = np.mean([jyp[job][yr] for yr in jyp[job] if yr != 2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834, 835, 838)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oe), len(op), len(ocg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/socName_aveMedianPay_aveMinEdu_socGroup.txt','w') as f:\n",
    "    for job in ocg:\n",
    "        if job in oe:\n",
    "            if job in op:\n",
    "                line = \"\\t\".join([job, str(op[job]), str(oe[job]), str(ocg[job])])\n",
    "            else:\n",
    "                line = \"\\t\".join([job, 'nan', str(oe[job]), str(ocg[job])])\n",
    "        else:\n",
    "            if job in op:\n",
    "                line = \"\\t\".join([job, str(op[job]), 'nan', str(ocg[job])])\n",
    "            else:\n",
    "                line = \"\\t\".join([job, 'nan', 'nan', str(ocg[job])])\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. year -company - N posts; 4a, 4b year - socName - skill freq for large and small company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract for each year: socName, jobid, employer from Raw Main Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in [2007, 2010, 2018, 2019]:\n",
    "    year = str(yr)\n",
    "    with open('/project2/jevans/ditong/data/SOC/'+year+'/'+year+'_jobid_employer.txt','w') as f:\n",
    "        pass\n",
    "    folder = '/project2/jevans/BG/Structured_Data/Main/' + year + '/'\n",
    "    for f in listdir(folder):\n",
    "        if not f.startswith('.'):\n",
    "            print(f)\n",
    "            with ZipFile(folder+f) as z:\n",
    "                with z.open(f.split('.')[0]+'.txt') as f1:\n",
    "                    df = pd.read_csv(f1,header='infer',delimiter=\"\\t\", encoding = \"ISO-8859-1\", low_memory=False)\n",
    "                    for a,b,c,d in zip(df['BGTJobId'],df['Employer'], df['SOCName'], df['MSA']): \n",
    "                        if b[-1] == '\\xa0':\n",
    "                            b = b[:-1]\n",
    "                        if b != 'na' and c != 'na' and d != -999:\n",
    "                            line = \"\\t\".join([str(a), str(b), str(c)])\n",
    "                        with open('/project2/jevans/ditong/data/SOC/'+year+'/'+year+'_jobid_employer_occ.txt','a') as f2:\n",
    "                            f2.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce 3-Company N Posts; 4a, 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.02010company number:  308864\n",
      "2010occupation number:  815 796\n",
      "216.02018company number:  988491\n",
      "2018occupation number:  831 822\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/lc10_occ_year_skill_freq.txt', 'w') as f1:\n",
    "    pass\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/sc10_occ_year_skill_freq.txt', 'w') as f2:\n",
    "    pass\n",
    "ycsize = defaultdict(lambda:defaultdict(lambda:0))\n",
    "for i in [2010, 2018]:\n",
    "    year = str(i)\n",
    "    # jobid-company\n",
    "    JE = {}\n",
    "    id_job={}\n",
    "    n = 0\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/' + year + '/' + \n",
    "              year + '_jobid_employer_occ.txt','r') as f:\n",
    "        for line in f:\n",
    "            n+=1\n",
    "            if n%100000==0:\n",
    "                flushPrint(n/100000)\n",
    "            line = line.strip('\\n').split('\\t')\n",
    "            JE[int(line[0])] = line[1]\n",
    "            id_job[int(line[0])] = line[2]\n",
    "            \n",
    "    # company size\n",
    "    for j in JE:\n",
    "        ycsize[year][JE[j]] += 1 \n",
    "    print(year + \"company number: \", len(ycsize[year]))\n",
    "    \n",
    "            \n",
    "    # read in the jobid-skill data               \n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/'+ year + '/' + year + '_jobid_date_skill.txt', 'r') as f:\n",
    "        columns = ['jobid', 'date', \"skills\"]\n",
    "        df = pd.read_csv(f, names=columns, delimiter=\"\\t\",encoding = \"ISO-8859-1\") \n",
    "        lcocc_year_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "        scocc_year_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "        for i,j in zip(df['jobid'],df['skills']): \n",
    "            if i in JE:\n",
    "                occ = id_job[i]\n",
    "                skills = j.split('_')[:-1]\n",
    "                if ycsize[year][JE[i]] > 10:\n",
    "                    for skill in skills:\n",
    "                        lcocc_year_skill_freq[occ][year][skill]+=1\n",
    "                else:\n",
    "                    for skill in skills:\n",
    "                        scocc_year_skill_freq[occ][year][skill]+=1\n",
    "                        \n",
    "    print(year+\"occupation number: \", len(lcocc_year_skill_freq), len(scocc_year_skill_freq))\n",
    "    \n",
    "    # save the skill frequency dictionary into txt file\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/lc10_occ_year_skill_freq.txt', 'a') as f3:\n",
    "        for occ in lcocc_year_skill_freq:\n",
    "            for year in lcocc_year_skill_freq[occ]:\n",
    "                skill_freq = \"\\t\".join([\"_\".join([skill,\n",
    "                                str(lcocc_year_skill_freq[occ][year][skill])]) for skill in lcocc_year_skill_freq[occ][year]])\n",
    "                line = \"\\t\".join([occ, year, skill_freq]) \n",
    "                f3.write(line + \"\\n\")\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/sc10_occ_year_skill_freq.txt', 'a') as f4:\n",
    "        for occ in scocc_year_skill_freq:\n",
    "            for year in scocc_year_skill_freq[occ]:\n",
    "                skill_freq = \"\\t\".join([\"_\".join([skill, \n",
    "                                str(scocc_year_skill_freq[occ][year][skill])]) for skill in scocc_year_skill_freq[occ][year]])\n",
    "                line = \"\\t\".join([occ, year, skill_freq]) \n",
    "                f4.write(line + \"\\n\")\n",
    "    \n",
    "#     # calculate the tf-idf score for each skill for each occupation, store in dicationary\n",
    "#     occ_skill_tfidf = defaultdict(lambda:defaultdict(lambda:0))\n",
    "#     n=0\n",
    "#     for occ in occ_skill_freq:\n",
    "#         n+=1\n",
    "#         flushPrint(n)\n",
    "#         for skill in occ_skill_freq[occ]:\n",
    "#             occ_skill_tfidf[occ][skill] = (occ_skill_freq[occ][skill] / len(occ_skill_freq[occ])) * math.log(len(occ_skill_freq) / len(skill_occnum[skill]))\n",
    "#     # sort the skills for each occupation by tf-idf value\n",
    "#     for occ in occ_skill_tfidf:\n",
    "#         sorted_dic = dict(sorted(occ_skill_tfidf[occ].items(), key=operator.itemgetter(1), reverse=True))\n",
    "#         occ_skill_tfidf[occ] = sorted_dic\n",
    "#     # save the dictionary into txt file\n",
    "#     with open('/Users/ditong/Documents/skillProject/description/data/SOC/lc25000_occ_year_skill_tfidf.txt', 'a') as f:\n",
    "#         for occ in occ_skill_tfidf:\n",
    "#             skill_idf = \"\\t\".join([\"_\".join([skill, str(occ_skill_tfidf[occ][skill])]) for skill in occ_skill_tfidf[occ]])\n",
    "#             line = \"\\t\".join([occ, year, skill_idf]) \n",
    "#             f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/CompanyNPost.txt', 'w') as f:\n",
    "    for year in ycsize:\n",
    "        for comp in ycsize[year]:\n",
    "            line = \"\\t\".join([year, comp, str(ycsize[year][comp])])\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycsize=defaultdict(lambda: defaultdict(lambda:0))\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/CompanyNPost.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        year = line.split(\"\\t\")[0]\n",
    "        comp = line.split(\"\\t\")[1]\n",
    "        postn = line.split(\"\\t\")[2]\n",
    "        ycsize[int(year)][comp] = int(postn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8., 10., 14.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(list(ycsize[2018].values()), [85, 87, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7., 10., 13.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(list(ycsize[2010].values()), [85, 88, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company size\n",
    "threshcs={}\n",
    "threshcs[2010]=np.percentile(list(ycsize[2010].values()), [90])[0]\n",
    "threshcs[2018]=np.percentile(list(ycsize[2018].values()), [90])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2010: 13.0, 2018: 14.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### socName - skill freq for large and small location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yidocc=defaultdict(lambda:defaultdict(lambda:''))\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/2010/2010_job_id.txt','r') as f:\n",
    "    for line in f:\n",
    "        occ=line.split('\\t')[0]\n",
    "        ids=line.split('\\t')[1].split('_')\n",
    "        for ind in ids:\n",
    "            yidocc[2010][int(ind)]=occ\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/2018/2018_job_id.txt','r') as f:\n",
    "    for line in f:\n",
    "        occ=line.split('\\t')[0]\n",
    "        ids=line.split('\\t')[1].split('_')\n",
    "        for ind in ids:\n",
    "            yidocc[2018][int(ind)]=occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11406899, 27955176)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yidocc[2010]), len(yidocc[2018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875.0"
     ]
    }
   ],
   "source": [
    "#location\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/lloc1_occ_year_skill_freq.txt', 'w') as f1:\n",
    "    pass\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/sloc1_occ_year_skill_freq.txt', 'w') as f2:\n",
    "    pass\n",
    "n=0\n",
    "yidoccloc = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "ylocsize = defaultdict(lambda:defaultdict(lambda:0))\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/jobid_state_city_loc.txt',\n",
    "          'r') as f:\n",
    "    for line in f:\n",
    "        n+=1\n",
    "        if n%100000==0:\n",
    "            flushPrint(n/100000)\n",
    "        ind = int(line.split(\"\\t\")[0])\n",
    "        locstr = line.split(\"\\t\")[3]\n",
    "        loc=(round(float(locstr.split(\"_\")[0]),1), round(float(locstr.split(\"_\")[1]),1))\n",
    "        if ind in yidocc[2010]:\n",
    "            yidoccloc[2010][ind]=(yidocc[2010][ind], loc)\n",
    "            ylocsize[2010][loc] +=1\n",
    "        elif ind in yidocc[2018]:\n",
    "            yidoccloc[2018][ind]=(yidocc[2018][ind], loc)\n",
    "            ylocsize[2018][loc] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875.0"
     ]
    }
   ],
   "source": [
    "loc_statecity=defaultdict(lambda:('na', 'na'))\n",
    "n=0\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/jobid_state_city_loc.txt',\n",
    "          'r') as f:\n",
    "    for line in f:\n",
    "        n+=1\n",
    "        if n%100000==0:\n",
    "            flushPrint(n/100000)\n",
    "        ind = int(line.split(\"\\t\")[0])\n",
    "        state=line.split(\"\\t\")[1]\n",
    "        city=line.split(\"\\t\")[2]\n",
    "        locstr = line.split(\"\\t\")[3]\n",
    "        loc=(round(float(locstr.split(\"_\")[0]),1), round(float(locstr.split(\"_\")[1]),1))\n",
    "        if type(state)==float or type(city)==float:\n",
    "            print(city, state)\n",
    "        if loc_statecity[loc][0] in ['na', 'nan']:\n",
    "            newstate=state\n",
    "        else:\n",
    "            newstate=loc_statecity[loc][0]\n",
    "        if loc_statecity[loc][1] in ['na', 'nan']:\n",
    "            newcity=city\n",
    "        else:\n",
    "            newcity=loc_statecity[loc][1]\n",
    "        loc_statecity[loc] = (newstate, newcity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in loc_statecity:\n",
    "    if loc_statecity[loc][0] in ['na', 'nan'] and loc_statecity[loc][1] in ['na', 'nan']:\n",
    "        del loc_statecity[loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27239"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loc_statecity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/loc_state_city.txt', 'w') as f1:\n",
    "    for loc in loc_statecity:\n",
    "        locstr = \"_\".join([str(loc[0]), str(loc[1])])\n",
    "        line = \"\\t\".join([locstr, loc_statecity[loc][0], loc_statecity[loc][1]])\n",
    "        f1.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/year_loc1_size.txt', 'w') as f1:\n",
    "    for yr in ylocsize:\n",
    "        for loc in ylocsize[yr]:\n",
    "            size = ylocsize[yr][loc]\n",
    "            locstr = \"_\".join([str(loc[0]), str(loc[1])])\n",
    "            line = \"\\t\".join([str(yr),locstr, str(size)])\n",
    "            f1.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/year_jobid_loc_occ.txt', 'w') as f1:\n",
    "    for yr in yidoccloc:\n",
    "        for ind in yidoccloc[yr]:\n",
    "            occ = yidoccloc[yr][ind][0]\n",
    "            loc = \"_\".join([str(yidoccloc[yr][ind][1][0]), str(yidoccloc[yr][ind][1][1])])\n",
    "            line = \"\\t\".join([str(yr), str(ind), loc])\n",
    "            f1.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh={}\n",
    "thresh[2010]=np.percentile(list(ylocsize[2010].values()), [90])[0]\n",
    "thresh[2018]=np.percentile(list(ylocsize[2018].values()), [90])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2010: 709.0, 2018: 1040.7999999999993}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.02010occupation number:  826 814\n",
      "271.02018occupation number:  1056 1040\n"
     ]
    }
   ],
   "source": [
    "for yr in [2010, 2018]:   \n",
    "    # read in the jobid-skill data               \n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/'+ str(yr) + '/' + str(yr) + '_jobid_date_skill.txt', 'r') as f:\n",
    "        columns = ['jobid', 'date', \"skills\"]\n",
    "        df = pd.read_csv(f, names=columns, delimiter=\"\\t\",encoding = \"ISO-8859-1\") \n",
    "        llococc_year_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "        slococc_year_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "        n=0\n",
    "        for i,j in zip(df['jobid'],df['skills']): \n",
    "            if n%100000==0:\n",
    "                flushPrint(n/100000)\n",
    "            n+=1\n",
    "            if i in yidoccloc[yr]:\n",
    "                occ = yidoccloc[yr][i][0]\n",
    "                loc = yidoccloc[yr][i][1]\n",
    "                skills = j.split('_')[:-1]\n",
    "                if ylocsize[yr][loc] > thresh[yr]:\n",
    "                    for skill in skills:\n",
    "                        llococc_year_skill_freq[occ][yr][skill]+=1\n",
    "                else:\n",
    "                    for skill in skills:\n",
    "                        slococc_year_skill_freq[occ][yr][skill]+=1\n",
    "                        \n",
    "    print(str(yr)+\"occupation number: \", len(llococc_year_skill_freq), len(slococc_year_skill_freq))\n",
    "    \n",
    "    # save the skill frequency dictionary into txt file\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/lloc1_occ_year_skill_freq.txt', 'a') as f3:\n",
    "        for occ in llococc_year_skill_freq:\n",
    "            for year in llococc_year_skill_freq[occ]:\n",
    "                skill_freq = \"\\t\".join([\"_\".join([skill,\n",
    "                                str(llococc_year_skill_freq[occ][year][skill])]) for skill in llococc_year_skill_freq[occ][year]])\n",
    "                line = \"\\t\".join([occ, str(year), skill_freq]) \n",
    "                f3.write(line + \"\\n\")\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/sloc1_occ_year_skill_freq.txt', 'a') as f4:\n",
    "        for occ in slococc_year_skill_freq:\n",
    "            for year in slococc_year_skill_freq[occ]:\n",
    "                skill_freq = \"\\t\".join([\"_\".join([skill, \n",
    "                                str(slococc_year_skill_freq[occ][year][skill])]) for skill in slococc_year_skill_freq[occ][year]])\n",
    "                line = \"\\t\".join([occ,str(year), skill_freq]) \n",
    "                f4.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market size by company size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n",
      "64.02018\n",
      "216.0"
     ]
    }
   ],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/lclloc_occ_year_skill_freq.txt', 'w') as f1:\n",
    "    pass\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/lcsloc_occ_year_skill_freq.txt', 'w') as f2:\n",
    "    pass\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/sclloc_occ_year_skill_freq.txt', 'w') as f3:\n",
    "    pass\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/scsloc_occ_year_skill_freq.txt', 'w') as f4:\n",
    "    pass\n",
    "\n",
    "yjcl=defaultdict(lambda:defaultdict(lambda:[]))\n",
    "for i in [2010, 2018]:\n",
    "    year = str(i)\n",
    "    yr=int(year)\n",
    "    # jobid-company\n",
    "    JE = {}\n",
    "    id_job={}\n",
    "    n = 0\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/' + year + '/' + \n",
    "              year + '_jobid_employer_occ.txt','r') as f:\n",
    "        print(year)\n",
    "        for line in f:\n",
    "            n+=1\n",
    "            if n%100000==0:\n",
    "                flushPrint(n/100000)\n",
    "            line = line.strip('\\n').split('\\t')\n",
    "            jid = int(line[0])\n",
    "            JE[jid] = line[1]\n",
    "            id_job[jid] = line[2]         \n",
    "                \n",
    "           \n",
    "    # read in the jobid-skill data               \n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/'+ year + '/' + year + '_jobid_date_skill.txt', 'r') as f:\n",
    "        columns = ['jobid', 'date', \"skills\"]\n",
    "        df = pd.read_csv(f, names=columns, delimiter=\"\\t\",encoding = \"ISO-8859-1\") \n",
    "        lcllococc_year_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "        lcslococc_year_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "        scllococc_year_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "        scslococc_year_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "        for i,j in zip(df['jobid'],df['skills']): \n",
    "            if i in JE:\n",
    "                occ = id_job[i]\n",
    "                skills = j.split('_')[:-1]\n",
    "                if i in yidoccloc[yr]:\n",
    "                    loc = yidoccloc[yr][i][1]\n",
    "                    yjcl[yr][i]=[JE[i], loc]\n",
    "                    if ycsize[yr][JE[i]] > threshcs[yr] and ylocsize[yr][loc] > thresh[yr]:\n",
    "                        for skill in skills:\n",
    "                            lcllococc_year_skill_freq[occ][yr][skill]+=1\n",
    "                    elif ycsize[yr][JE[i]] > threshcs[yr] and ylocsize[yr][loc] <= thresh[yr]:\n",
    "                        for skill in skills:\n",
    "                            lcslococc_year_skill_freq[occ][yr][skill]+=1\n",
    "                    elif ycsize[yr][JE[i]] <= threshcs[yr] and ylocsize[yr][loc] > thresh[yr]:\n",
    "                        for skill in skills:\n",
    "                            scllococc_year_skill_freq[occ][yr][skill]+=1\n",
    "                    else:\n",
    "                        for skill in skills:\n",
    "                            scslococc_year_skill_freq[occ][yr][skill]+=1\n",
    "\n",
    "    \n",
    "    #save the skill frequency dictionary into txt file\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/lclloc_occ_year_skill_freq.txt', 'a') as f1:\n",
    "        for occ in lcllococc_year_skill_freq:\n",
    "            for yr in lcllococc_year_skill_freq[occ]:\n",
    "                skill_freq = \"\\t\".join([\"_\".join([skill,\n",
    "                                str(lcllococc_year_skill_freq[occ][yr][skill\n",
    "                                                ])]) for skill in lcllococc_year_skill_freq[occ][yr]])\n",
    "                line = \"\\t\".join([occ, str(yr), skill_freq]) \n",
    "                f1.write(line + \"\\n\")\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/lcsloc_occ_year_skill_freq.txt', 'a') as f2:\n",
    "        for occ in lcslococc_year_skill_freq:\n",
    "            for yr in lcslococc_year_skill_freq[occ]:\n",
    "                skill_freq = \"\\t\".join([\"_\".join([skill,\n",
    "                                str(lcslococc_year_skill_freq[occ][yr][skill\n",
    "                                                ])]) for skill in lcslococc_year_skill_freq[occ][yr]])\n",
    "                line = \"\\t\".join([occ, str(yr), skill_freq]) \n",
    "                f2.write(line + \"\\n\")\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/sclloc_occ_year_skill_freq.txt', 'a') as f3:\n",
    "        for occ in scllococc_year_skill_freq:\n",
    "            for yr in scllococc_year_skill_freq[occ]:\n",
    "                skill_freq = \"\\t\".join([\"_\".join([skill, \n",
    "                                str(scllococc_year_skill_freq[occ][yr][skill\n",
    "                                                ])]) for skill in scllococc_year_skill_freq[occ][yr]])\n",
    "                line = \"\\t\".join([occ, str(yr), skill_freq]) \n",
    "                f3.write(line + \"\\n\")\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/scsloc_occ_year_skill_freq.txt', 'a') as f4:\n",
    "        for occ in scslococc_year_skill_freq:\n",
    "            for yr in scslococc_year_skill_freq[occ]:\n",
    "                skill_freq = \"\\t\".join([\"_\".join([skill, \n",
    "                                str(scslococc_year_skill_freq[occ][yr][skill\n",
    "                                                ])]) for skill in scslococc_year_skill_freq[occ][yr]])\n",
    "                line = \"\\t\".join([occ, str(yr), skill_freq]) \n",
    "                f4.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n",
      "64.02018\n",
      "216.0"
     ]
    }
   ],
   "source": [
    "yjcl=defaultdict(lambda:defaultdict(lambda:[]))\n",
    "for i in [2010, 2018]:\n",
    "    year = str(i)\n",
    "    yr=int(year)\n",
    "    # jobid-company\n",
    "    JE = {}\n",
    "    id_job={}\n",
    "    n = 0\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/' + year + '/' + \n",
    "              year + '_jobid_employer_occ.txt','r') as f:\n",
    "        print(year)\n",
    "        for line in f:\n",
    "            n+=1\n",
    "            if n%100000==0:\n",
    "                flushPrint(n/100000)\n",
    "            line = line.strip('\\n').split('\\t')\n",
    "            jid = int(line[0])\n",
    "            JE[jid] = line[1]\n",
    "            id_job[jid] = line[2]         \n",
    "                \n",
    "           \n",
    "    # read in the jobid-skill data               \n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/'+ year + '/' + year + '_jobid_date_skill.txt', 'r') as f:\n",
    "        columns = ['jobid', 'date', \"skills\"]\n",
    "        df = pd.read_csv(f, names=columns, delimiter=\"\\t\",encoding = \"ISO-8859-1\") \n",
    "        for i,j in zip(df['jobid'],df['skills']): \n",
    "            if i in JE:\n",
    "                occ = id_job[i]\n",
    "                skills = j.split('_')[:-1]\n",
    "                if i in yidoccloc[yr]:\n",
    "                    loc = yidoccloc[yr][i][1]\n",
    "                    yjcl[yr][i]=[JE[i], loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/year_jobid_comp_loc.txt', 'w') as f1:\n",
    "    for yr in yjcl:\n",
    "        for ind in yjcl[yr]:\n",
    "            comp = yjcl[yr][ind][0]\n",
    "            loc = \"_\".join([str(yjcl[yr][ind][1][0]), str(yjcl[yr][ind][1][1])])\n",
    "            line = \"\\t\".join([str(yr), str(ind), comp, loc])\n",
    "            f1.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n",
      "61.02018\n",
      "203.0"
     ]
    }
   ],
   "source": [
    "# employer concentration\n",
    "LocYOccEmpNpost=defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0))))\n",
    "for yr in yjcl:\n",
    "    n=0\n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/' + str(yr) + '/' + \n",
    "              str(yr) + '_jobid_employer_occ.txt','r') as f:\n",
    "        print(yr)\n",
    "        for line in f:\n",
    "            n+=1\n",
    "            if n%100000==0:\n",
    "                flushPrint(n/100000)\n",
    "            line = line.strip('\\n').split('\\t')\n",
    "            jid = int(line[0])\n",
    "            id_job[jid] = line[2]\n",
    "    k=0     \n",
    "    for ind in yjcl[yr]:\n",
    "        k+=1\n",
    "        if k%100000==0:\n",
    "            flushPrint(k/100000)\n",
    "        comp = yjcl[yr][ind][0]\n",
    "        loc = (yjcl[yr][ind][1][0], yjcl[yr][ind][1][1])\n",
    "        LocYOccEmpNpost[loc][yr][id_job[ind]][comp] += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25301"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LocYOccEmpNpost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOccLocEC=defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "for loc in LocYOccEmpNpost:\n",
    "    for yr in LocYOccEmpNpost[loc]:\n",
    "        for occ in LocYOccEmpNpost[loc][yr]:\n",
    "            jsum=np.sum(list(LocYOccEmpNpost[loc][yr][occ].values()))\n",
    "            YOccLocEC[yr][occ][loc] = np.sum(\n",
    "                [(LocYOccEmpNpost[loc][yr][occ][emp] / jsum)**2 for emp in LocYOccEmpNpost[loc][yr][occ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 832)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(YOccLocEC[2010]), len(YOccLocEC[2018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/year_occ_loc_empcon.txt', 'w') as f1:\n",
    "    for yr in YOccLocEC:\n",
    "        for occ in YOccLocEC[yr]:\n",
    "            for loc in YOccLocEC[yr][occ]:\n",
    "                loc_str = \"_\".join([str(loc[0]), str(loc[1])])\n",
    "                line = \"\\t\".join([str(yr), str(occ), loc_str, str(YOccLocEC[yr][occ][loc])])\n",
    "                f1.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lloc_YOCCEC=defaultdict(lambda:defaultdict(lambda: []))\n",
    "Sloc_YOCCEC=defaultdict(lambda:defaultdict(lambda: []))\n",
    "for yr in YOccLocEC:\n",
    "    for occ in YOccLocEC[yr]:\n",
    "        for loc in YOccLocEC[yr][occ]:\n",
    "            if ylocsize[yr][loc] > thresh[yr]:\n",
    "                Lloc_YOCCEC[yr][occ].append(YOccLocEC[yr][occ][loc])\n",
    "            else:\n",
    "                Sloc_YOCCEC[yr][occ].append(YOccLocEC[yr][occ][loc])  \n",
    "        Lloc_YOCCEC[yr][occ]=np.mean(Lloc_YOCCEC[yr][occ])\n",
    "        Sloc_YOCCEC[yr][occ]=np.mean(Sloc_YOCCEC[yr][occ])\n",
    "        \n",
    "    Lloc_YOCCEC[yr]=dict(sorted(Lloc_YOCCEC[yr].items(), key=operator.itemgetter(1), reverse=True))\n",
    "    Sloc_YOCCEC[yr]=dict(sorted(Sloc_YOCCEC[yr].items(), key=operator.itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/LLOCyear_occ_empcon.txt', 'w') as f1:\n",
    "    for yr in Lloc_YOCCEC:\n",
    "        for occ in Lloc_YOCCEC[yr]:\n",
    "            line = \"\\t\".join([str(yr), str(occ), str(Lloc_YOCCEC[yr][occ])])\n",
    "            f1.write(line + \"\\n\")\n",
    "\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/SLOCyear_occ_empcon.txt', 'w') as f1:\n",
    "    for yr in Sloc_YOCCEC:\n",
    "        for occ in Sloc_YOCCEC[yr]:\n",
    "            line = \"\\t\".join([str(yr), str(occ), str(Sloc_YOCCEC[yr][occ])])\n",
    "            f1.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. year - socName - skill freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract 10 * Year: jobid, year, skill from Raw Skill Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "for i in range(2010, 2020):\n",
    "    year = str(i)\n",
    "    with open('/project2/jevans/ditong/data/SOC/'+ year + '/' + year + '_jobid_date_skill.txt', 'w') as f:\n",
    "        pass\n",
    "    folder = '/project2/jevans/BG/Structured_Data/Skill_zip/' + year + '/'\n",
    "    for f in listdir(folder):\n",
    "        if not f.startswith('.'):\n",
    "            print(f)\n",
    "            with ZipFile(folder+f) as z:\n",
    "                with z.open(f.split('.')[0]+'.txt') as f1:\n",
    "                    df = pd.read_csv(f1,header='infer',delimiter=\"\\t\",encoding = \"ISO-8859-1\")\n",
    "                    filtered_dic = defaultdict(lambda:defaultdict(lambda:''))\n",
    "                    for i,j,k in zip(df['BGTJobId'],df['JobDate'],df['Skill']): \n",
    "                        if k != 'na':\n",
    "                            filtered_dic[i][j]=filtered_dic[i][j] +  k + '_'\n",
    "            with open('/project2/jevans/ditong/data/SOC/'+ year + '/' + year + '_jobid_date_skill.txt', 'a') as f:\n",
    "                for jobid in filtered_dic:\n",
    "                    for date in filtered_dic[jobid]:\n",
    "                        skill = filtered_dic[jobid][date]\n",
    "                        line = \"\\t\".join([str(jobid), date, skill]) \n",
    "                        f.write(line + \"\\n\")\n",
    "                    \n",
    "elapsed = (time.clock() - start)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Year - SOCName - Skill Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2010, 2020):\n",
    "    year = str(i)\n",
    "    # read in the jobid-occupation name data\n",
    "    dic_id_job = {}\n",
    "    with open('/project2/jevans/ditong/data/SOC/' + year + '/' + \n",
    "              year + '_job_id.txt','r') as f:\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                line_lst = line[:-1].split('\\t')\n",
    "                job = line_lst[0]\n",
    "                ids = line_lst[1].split(\"_\")\n",
    "                for jobid in ids:\n",
    "                    dic_id_job[int(jobid)] = job\n",
    "                    \n",
    "    # read in the jobid-skill data               \n",
    "    with open('/project2/jevans/ditong/data/SOC/'+ year + '/' + year + '_jobid_date_skill.txt', 'r') as f:\n",
    "        print(f)\n",
    "        columns = ['jobid', 'date', \"skills\"]\n",
    "        df = pd.read_csv(f, names=columns, delimiter=\"\\t\",encoding = \"ISO-8859-1\") \n",
    "        occ_skill_freq = defaultdict(lambda:defaultdict(lambda:0))\n",
    "        skill_occnum = defaultdict(lambda:set())\n",
    "        for i,j in zip(df['jobid'],df['skills']): \n",
    "            if i in dic_id_job:\n",
    "                occ = dic_id_job[i]\n",
    "                skills = j.split('_')[:-1]\n",
    "                for skill in skills:\n",
    "                    occ_skill_freq[occ][skill]+=1\n",
    "                    skill_occnum[skill].add(occ)\n",
    "    print(len(occ_skill_freq), len(skill_occnum))\n",
    "    \n",
    "    # save the skill frequency dictionary into txt file\n",
    "    with open('/project2/jevans/ditong/data/SOC/'+ year + '/' + year + '_occ_skill_freq.txt', 'w') as f:\n",
    "        for occ in occ_skill_freq:\n",
    "            skill_freq = \"\\t\".join([\"_\".join([skill, str(occ_skill_freq[occ][skill])]) for skill in occ_skill_freq[occ]])\n",
    "            line = \"\\t\".join([occ, skill_freq]) \n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    \n",
    "#     # calculate the tf-idf score for each skill for each occupation, store in dicationary\n",
    "#     occ_skill_tfidf = defaultdict(lambda:defaultdict(lambda:0))\n",
    "#     for occ in occ_skill_freq:\n",
    "#         for skill in occ_skill_freq[occ]:\n",
    "#             occ_skill_tfidf[occ][skill] = (occ_skill_freq[occ][skill] / len(occ_skill_freq[occ])) * math.log(len(occ_skill_freq) / len(skill_occnum[skill]))\n",
    "#     # sort the skills for each occupation by tf-idf value\n",
    "#     for occ in occ_skill_tfidf:\n",
    "#         sorted_dic = dict(sorted(occ_skill_tfidf[occ].items(), key=operator.itemgetter(1), reverse=True))\n",
    "#         occ_skill_tfidf[occ] = sorted_dic\n",
    "#     # save the dictionary into txt file\n",
    "#     with open('/project2/jevans/ditong/data/SOC/'+ year + '/' + year + '_occ_skill_tfidf.txt', 'w') as f:\n",
    "#         for occ in occ_skill_tfidf:\n",
    "#             skill_idf = \"\\t\".join([\"_\".join([skill, str(occ_skill_tfidf[occ][skill])]) for skill in occ_skill_tfidf[occ]])\n",
    "#             line = \"\\t\".join([occ, skill_idf]) \n",
    "#             f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all Years in One file\n",
    "with open ('/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/occ_year_skill_freq.txt', 'w') as f:\n",
    "    for i in range(2010, 2020):\n",
    "        year = str(i)\n",
    "        with open('/project2/jevans/ditong/data/SOC/'+ year + '/' + year + '_occ_skill_freq.txt', 'r') as f1:\n",
    "            for line in f1:\n",
    "                if line != '\\n':\n",
    "                    line_lst = line.split(\"\\t\")\n",
    "                    occ = line_lst[0]\n",
    "                    skills = line_lst[1:]\n",
    "                    newline = \"\\t\".join([occ, year]+skills)\n",
    "                    f.write(newline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Replication of Deming and Noray's (2020) Skill Change Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for replicating Deming and Noray's measurement: use sample with non missing employer and MSA\n",
    "\n",
    "# sample with non-missing employer and MSA\n",
    "# prepare and save data: occupation - year - skill - frequency; occupation - year - npost; occupation - year - sum skill frequency\n",
    "occ_y_sfreq = defaultdict(lambda:defaultdict(lambda:0))\n",
    "occ_y_freq = defaultdict(lambda:defaultdict(lambda:0))\n",
    "occ_y_skill_freq = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda:0)))\n",
    "\n",
    "for year in ['2007', '2019', '2010', '2018']:\n",
    "    dic_id_job = {}\n",
    "    # read in the jobid-occupation name data\n",
    "    if year in ['2007', '2019']:\n",
    "        path = '/Users/ditong/Dropbox (MIT)/skills/Data/raw/' + year + '_jobid_employer_occ.txt'\n",
    "    elif year in ['2010', '2018']:\n",
    "        path = '/Users/ditong/Dropbox (MIT)/skills/Data/raw/' + year + '/' + year + '_jobid_employer_occ.txt'\n",
    "        \n",
    "    with open(path,'r') as f:\n",
    "        n=0\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                n+=1\n",
    "                if n%100000 == 0:\n",
    "                    flushPrint(n/100000)\n",
    "                line_lst = line.strip('\\n').split('\\t')\n",
    "                dic_id_job[int(line_lst[0])] = line_lst[2]\n",
    "                \n",
    "    \n",
    "    print(year, \": \", len(dic_id_job))\n",
    "    \n",
    "    # read in the jobid-skill data               \n",
    "    with open('/Users/ditong/Dropbox (MIT)/skills/Data/raw/' + year + '/' + year + '_jobid_date_skill.txt', 'r') as f:\n",
    "        columns = ['jobid', 'date', \"skills\"]\n",
    "        df = pd.read_csv(f, names=columns, delimiter=\"\\t\",encoding = \"ISO-8859-1\", low_memory=False) \n",
    "        m=0\n",
    "        for i,j in zip(df['jobid'],df['skills']): \n",
    "            m+=1\n",
    "            if m%100000 == 0:\n",
    "                flushPrint(m/100000)\n",
    "            if i in dic_id_job:\n",
    "                occ = dic_id_job[i]\n",
    "                skills = j.split('_')[:-1]\n",
    "                for s in skills:\n",
    "                    occ_y_sfreq[occ][year]+=1\n",
    "                    occ_y_skill_freq[occ][year][s]+=1\n",
    "                occ_y_freq[occ][year]+=1\n",
    "\n",
    "print(len(occ_y_sfreq), len(occ_y_freq))\n",
    "\n",
    "# save data\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/SkillPaper/IntermediateData/occ_year_skill_freq_emp.txt', 'w') as f:\n",
    "    for occ in occ_y_skill_freq:\n",
    "        for year in occ_y_skill_freq[occ]:\n",
    "            sfreqs = []\n",
    "            for s in occ_y_skill_freq[occ][year]:\n",
    "                sfreqs.append(\"_\".join([s, str(occ_y_skill_freq[occ][year][s])]))\n",
    "            line = \"\\t\".join([occ, year] + sfreqs)\n",
    "            f.write(line + '\\n')\n",
    "            \n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/SkillPaper/IntermediateData/occ_year_npost_emp.txt', 'w') as f:\n",
    "    for occ in occ_y_freq:\n",
    "        for year in occ_y_freq[occ]:\n",
    "            line = \"\\t\".join([occ, year, str(occ_y_freq[occ][year])])\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate job skill change and save data\n",
    "# weights\n",
    "jw1 = {}\n",
    "for j in occ_y_sfreq:\n",
    "    if j in occ_y_freq:\n",
    "        if '2010' in occ_y_sfreq[j] and '2018' in occ_y_sfreq[j]:\n",
    "            jw1[j] = (occ_y_sfreq[j]['2010'] / occ_y_sfreq[j]['2018']) / (occ_y_freq[j]['2010']/occ_y_freq[j]['2018'])\n",
    "            if jw1[j] > 1:\n",
    "                jw1[j] = 1\n",
    "                \n",
    "jw2 = {}\n",
    "for j in occ_y_sfreq:\n",
    "    if j in occ_y_freq:\n",
    "        if '2007' in occ_y_sfreq[j] and '2019' in occ_y_sfreq[j]:\n",
    "            jw2[j] = (occ_y_sfreq[j]['2007'] / occ_y_sfreq[j]['2019']) / (occ_y_freq[j]['2007']/occ_y_freq[j]['2019'])\n",
    "            if jw2[j] > 1:\n",
    "                jw2[j] = 1\n",
    "                \n",
    "# skill probability\n",
    "jyspro = copy.deepcopy(occ_y_skill_freq)\n",
    "for j in jyspro:\n",
    "    for y in jyspro[j]:\n",
    "        for s in jyspro[j][y].keys():\n",
    "            jyspro[j][y][s] = jyspro[j][y][s]/occ_y_freq[j][y]\n",
    "            \n",
    "# job change\n",
    "jyc = defaultdict(lambda:defaultdict(lambda:0))\n",
    "occsc79=defaultdict(lambda:0)\n",
    "occsc80=defaultdict(lambda:0)\n",
    "for j in jyspro:\n",
    "    if '2007' in jyspro[j] and '2019' in jyspro[j]:\n",
    "        # get the union of skills of 2 years\n",
    "        alls = set(jyspro[j]['2007'].keys()).union(set(jyspro[j]['2019'].keys()))\n",
    "        # skill probability change\n",
    "        for s in alls:\n",
    "            jyc[j]['07-19'] += abs(jyspro[j]['2019'][s] - jyspro[j]['2007'][s])\n",
    "        jyc[j]['07-19'] = jyc[j]['07-19']*jw2[j]\n",
    "        occsc79[j] = jyc[j]['07-19']\n",
    "            \n",
    "    if '2010' in jyspro[j] and '2018' in jyspro[j]:\n",
    "        # get the union of skills of 2 years\n",
    "        alls = set(jyspro[j]['2010'].keys()).union(set(jyspro[j]['2018'].keys()))\n",
    "        # skill probability change\n",
    "        for s in alls:\n",
    "            jyc[j]['10-18'] += abs(jyspro[j]['2018'][s] - jyspro[j]['2010'][s])\n",
    "        jyc[j]['10-18'] = jyc[j]['10-18']*jw1[j]\n",
    "        occsc80[j] = jyc[j]['10-18']\n",
    "        \n",
    "# save \n",
    "df79 = pd.DataFrame()\n",
    "sorted79 =  dict(sorted(occsc79.items(), key=operator.itemgetter(1), reverse=True))\n",
    "df79['SOC'] = list(sorted79.keys())\n",
    "df79[\"SkillChange\"] = list(sorted79.values())\n",
    "df79.to_csv('/Users/ditong/Dropbox (MIT)/skills/SkillPaper/IntermediateData/0719Soc6SkillChange_emp.csv')\n",
    "\n",
    "df80 = pd.DataFrame()\n",
    "sorted80 =  dict(sorted(occsc80.items(), key=operator.itemgetter(1), reverse=True))\n",
    "df80['SOC'] = list(sorted80.keys())\n",
    "df80[\"SkillChange\"] = list(sorted80.values())\n",
    "df80.to_csv('/Users/ditong/Dropbox (MIT)/skills/SkillPaper/IntermediateData/1018Soc6SkillChange_emp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. skill pair PMI calculation for network building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skill-occupation-frequency dict\n",
    "skillof = defaultdict(lambda:defaultdict(lambda:'na'))\n",
    "with open('/Users/ditong/Dropbox (MIT)/skills/SkillPaper/IntermediateData/occ_year_skill_freq.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line_lst = line.split(\"\\t\")\n",
    "        year = int(line_lst[1])\n",
    "        if year == 2010:\n",
    "            job = line_lst[0]\n",
    "            skills = line_lst[2:]\n",
    "            for skill_freq in skills:\n",
    "                skill = skill_freq.split('_')[0].lower()\n",
    "                if skill in oskill[job]:\n",
    "                    freq = int(skill_freq.split('_')[1])\n",
    "                    skillof[skill][job] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change skill name into skill indices, create mapping dict\n",
    "# create occupation-skill-frequency, skill-occupation-frequency and occupation-skill list dicts with skill indices \n",
    "s_id_n = {}\n",
    "osf = defaultdict(lambda:defaultdict(lambda:'na'))\n",
    "sof = defaultdict(lambda:defaultdict(lambda:'na'))\n",
    "os = defaultdict(lambda:[])\n",
    "n=0\n",
    "for s in skillof:\n",
    "    s_id_n[n] = s\n",
    "    for o in skillof[s]:\n",
    "        os[o].append(n)\n",
    "        osf[o][n] = skillof[s][o]\n",
    "        sof[n][o] = skillof[s][o]\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the skill name-indices dictionary\n",
    "with open('/Users/ditong/Documents/large scale computing/final-project-di-Tong/2010_skill_id_name.json', 'w') as fp:\n",
    "    json.dump(s_id_n, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call a AWS Lambda function to find skill pairs co-occuring in the same occupation \n",
    "# for a subset of occupations\n",
    "def find_skill_pairs(lsts_occs_skills):\n",
    "    '''\n",
    "    Input: \n",
    "        lsts_occs_skills: a list of lists--a list of occupations, each as a skill list\n",
    "    \n",
    "    Returns:\n",
    "        sets of co-occuring skill pairs (skill1, skill2)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    skill_pair_set = set()\n",
    "    # input (list of list): a list of occupations, each as a skill list\n",
    "    for lst_skills in lsts_occs_skills:\n",
    "        # get all pairs in each skill list\n",
    "        skill_pairs = [(lst_skills[i],lst_skills[j]) for i in range(len(lst_skills)) \n",
    "                   for j in range(i+1, len(lst_skills))]\n",
    "        # sort the pairs to filter out duplicates in the merging process\n",
    "        sorted_pairs = [tuple(sorted(p)) for p in skill_pairs]\n",
    "        # merge the resulting pairs across all the occupations\n",
    "        skill_pair_set = skill_pair_set.union(set(sorted_pairs))\n",
    "          \n",
    "    return skill_pair_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let each aws lambda function find co-occuring skill pairs in a subset of 40 occupations\n",
    "lsts_skills = list(os.values())\n",
    "split = [lsts_skills[x:x+40] for x in range(0, len(lsts_skills), 40)]\n",
    "# Set up a pywren executor and find co-occuring skill pairs\n",
    "pwex = pywren.default_executor()\n",
    "skill_pairs = pywren.get_all_results(pwex.map(find_skill_pairs, split))\n",
    "\n",
    "# merge all skill pairs into one list (duplicates deleted in process)\n",
    "sp = set()\n",
    "n=0\n",
    "for skill_pair in skill_pairs:\n",
    "    n+=1\n",
    "    flushPrint(n)\n",
    "    sp = sp.union(skill_pair)\n",
    "sp=list(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data: sum frequncy for all skills for all occupation; occupation-probablity dict\n",
    "sum_freq = sum([sum(v.values()) for k,v in osf.items()])\n",
    "oprob = {}\n",
    "for occ in osf:\n",
    "    sum_occ_freq = sum(osf[occ].values())\n",
    "    oprob[occ] = sum_occ_freq / sum_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call a AWS Lambda function to calculate skill pairs PMI for a subset of skill pairs\n",
    "def calculate_sp_pmi(lst_pairs, sof, oprob, sum_freq):\n",
    "    '''\n",
    "    Input: \n",
    "        lst_pairs: a list of skill pairs (in tuple)\n",
    "        sof: skill-occupation-frequency dictionary\n",
    "        oprob: occupation-probability dictionary\n",
    "        sum_freq: the sum of frequncy for all skills in all occupations (the sum of all values in sof)\n",
    "    \n",
    "    Formula:\n",
    "    skill pair (i, j)'s PMI<i, j> = log(P<i,j> / (Pi * Pj))\n",
    "    P<i,j> = sum (P<i|c> * P<j|c> * Pc) for all occupations c\n",
    "    P<i|c> = P<i,c> / Pc\n",
    "    P<i,c> = Freq<i,c> / sum_freq\n",
    "    Pi = Freq<i> / sum_freq\n",
    "    \n",
    "    Returns:\n",
    "        lists of (skill1, skill2, PMI)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    pair_pmi = []\n",
    "    for i,j in lst_pairs:\n",
    "        if i in sof and j in sof:\n",
    "            pij = 0\n",
    "            # find the occupations that i and j are co-occuring\n",
    "            occs = set(sof[i].keys()).intersection(set(sof[j].keys()))\n",
    "            for occ in occs:\n",
    "                # check if the occupation has a probability value\n",
    "                if occ in oprob:\n",
    "                    # i and j's co-occuring probability in each occupation weighted by occupation's probability\n",
    "                    pijc = (sof[i][occ] / sum_freq) * (sof[j][occ] / sum_freq) / oprob[occ]\n",
    "                    # sum up i and j's weighted co-occuring probabilities in all occupations\n",
    "                    pij += pijc\n",
    "            # skill i 's probability\n",
    "            pi = sum(sof[i].values()) / sum_freq\n",
    "            # skill j 's probability\n",
    "            pj = sum(sof[j].values()) / sum_freq\n",
    "            # pmi-skill i and j\n",
    "            pmi_ij = np.log(pij / (pi * pj))\n",
    "            pair_pmi.append((i, j, pmi_ij))\n",
    "                   \n",
    "    return pair_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into chunks with 50000 skill pairs each \n",
    "# ( I found after trials that each pywren call could not execute more data than this on my task)\n",
    "splits = [sp[x:x+50000] for x in range(0, len(sp), 50000)]\n",
    "# set up am empty list to merge returned data from each pywren call\n",
    "sp_pmis = []\n",
    "# set up an empty file to write in data returned from each pywren call\n",
    "with open('./skillpair_pmi.txt', 'w') as f:\n",
    "    pass\n",
    "# call pywren to Calculate Pairwise Skill Pmi for each chunk of data\n",
    "n=0\n",
    "for splitk in splits[180:]:\n",
    "    # let each aws lambda function Calculate Pairwise Skill Pmi for a subset of 50 skill pairs\n",
    "    split = [splitk[x:x+50] for x in range(0, len(splitk), 50)]\n",
    "    # Set up a pywren executor\n",
    "    pwex = pywren.default_executor()\n",
    "    sp_pmi = pywren.get_all_results(pwex.map(lambda x: calculate_sp_pmi(x, sof, oprob, sum_freq), split))\n",
    "    sp_pmis += sp_pmi\n",
    "    with open('./skillpair_pmi.txt', 'a') as f:\n",
    "        for sppmi in sp_pmi:\n",
    "            for i, j, pmi in sppmi:\n",
    "                line = \"\\t\".join([str(i), str(j), str(pmi)])\n",
    "                f.write(line + \"\\n\")\n",
    "    flushPrint(n)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. skill embedding (all years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract from Raw Skill Data: 10 * Year Skill Co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skill(gr): \n",
    "    cleaned_gr = [element.lower() for element in gr if element != 'na']\n",
    "    cleaned_gr = list(set(cleaned_gr))\n",
    "    skills = '\\t'.join(cleaned_gr)\n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df, f1):\n",
    "    grouped = df.groupby('BGTJobId')['Skill'].apply(extract_skill)\n",
    "    skills = []\n",
    "    seed = 0\n",
    "    for job in grouped:\n",
    "        seed += 1\n",
    "        skill_lst = job.split('\\t')\n",
    "        if skill_lst != ['']:\n",
    "            if len(skill_lst) > 2:\n",
    "                final_lst = list(permutation(skill_lst))\n",
    "            else:\n",
    "                final_lst = skill_lst\n",
    "            line = \"\\t\".join(final_lst)\n",
    "            f1.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2018, 2020):\n",
    "    start = time.clock()\n",
    "    print(year)\n",
    "    with open('/project2/jevans/ditong/skill_embed_imput/' + str(year) + '_skills.txt','wb') as f1:\n",
    "        folder='/project2/jevans/BG/Structured Data/Skill/'+str(year)+'/'\n",
    "        for f in listdir(folder):\n",
    "            if not f.startswith('.'):\n",
    "                print(f)\n",
    "                with ZipFile(folder+f) as z:\n",
    "                    with z.open(f.split('.')[0]+'.txt') as f:\n",
    "                        df = pd.read_csv(f,header='infer',delimiter=\"\\t\",encoding = \"ISO-8859-1\")\n",
    "                        pre_process(df, f1)\n",
    "        elapsed = (time.clock() - start)\n",
    "        print(elapsed/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce Skill Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data streaming to prepare for imput\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            with open(self.dirname + fname,'r') as f:\n",
    "                for line in f:\n",
    "                    yield line[:-1].split('\\t')\n",
    "sentences = MySentences('/project2/jevans/ditong/skill_embed_imput/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only include skills with at least 10 occurences; vector dimension 200; window size 300 to be larger than\n",
    "# the largest skill number required in one post; \n",
    "start = time.clock()\n",
    "model_1 = Word2Vec(sentences, min_count=10,size= 200, workers=8, window=300, sg = 1, negative=5, compute_loss=True)\n",
    "elapsed = (time.clock() - start)\n",
    "model_1.get_latest_training_loss()\n",
    "model_1.save(\"/Users/ditong/Dropbox (MIT)/skills/Data/intermediate/w2v_m10_s200_w300_sg1_n5.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. skill atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_aksvd(w2vmodel, n_comp, n_nonzeros, save=False, savelocation='/Users/ditong/Dropbox (MIT)/skills/atom_model/'): \n",
    "    #set for Alina's files, switch to where you want save the model\n",
    "    #https://github.com/nel215/ksvd #takes about 2 min on Alina's laptop for 30 atoms \n",
    "    aksvd_t = ApproximateKSVD(n_components=n_comp, transform_n_nonzero_coefs=n_nonzeros) \n",
    "    #also may adjuste n iter which is default at 10, and tolerance for error which is default at  tol=1e-6 \n",
    "    #n_components is number of discourse atoms, since vocab size is smallish, keep this fewer. \n",
    "    #transform_n is the number of atoms (components) that a word can be a linear combo of\n",
    "    \n",
    "    dictionary_t = aksvd_t.fit(w2vmodel.wv.vectors).components_ \n",
    "    #Dictionary is the matrix of discourse atoms. \n",
    "    alpha_t = aksvd_t.transform(w2vmodel.wv.vectors) \n",
    "    #get the alphas, which are the \"weights\" of each word on a discourse atoms\n",
    "\n",
    "    if save==True:\n",
    "        outfile = open(str(savelocation) + '200d_' + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_aksvd_nvdrsdf20','wb')\n",
    "        pickle.dump(aksvd_t,outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "        outfile = open(str(savelocation) + '200d_' +str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_dictionary_nvdrsdf20','wb')\n",
    "        pickle.dump(dictionary_t,outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "        outfile = open(str(savelocation) + '200d_' + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_alpha_nvdrsdf20','wb')\n",
    "        pickle.dump(alpha_t,outfile)\n",
    "        outfile.close()\n",
    "    return(dictionary_t, alpha_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconst_qual(w2vmodel, dictionary_mat, alpha_mat):\n",
    "    #reconstruct the word vectors\n",
    "    reconstructed = alpha_mat.dot(dictionary_mat) #reconstruct word vectors and add back in mean(?). but note that reconstructed norm is still around 0-1, not 1, is that an issue?\n",
    "    #e1 = norm(w2vmodel.wv.vectors - reconstructed) #total reconstruction error, larger means MORE error. norm as specified here takes frobenius norm of error matrix.\n",
    "\n",
    "\n",
    "    #total VARIANCE in the data: sum of squares \n",
    "    squares3= w2vmodel.wv.vectors-np.mean(w2vmodel.wv.vectors, axis=1).reshape(-1,1) #https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/\n",
    "    #sst3= np.sum([i.dot(i) for i in squares3] ) #same as below\n",
    "\n",
    "    sst3= np.sum(np.square(squares3))\n",
    "\n",
    "\n",
    "    #total sum of squared ERRORS/residuals\n",
    "    e3= [reconstructed[i]-w2vmodel.wv.vectors[i] for i in range(0,len(w2vmodel.wv.vectors))]  #https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/\n",
    "    #sse3= np.sum([i.dot(i) for i in e3] ) #same as below\n",
    "    sse3= np.sum(np.square(e3))\n",
    "\n",
    "    #R^2: 1- (SSE / SST )\n",
    "    r2= 1- (sse3 /  sst3) #https://stats.stackexchange.com/questions/184603/in-pca-what-is-the-connection-between-explained-variance-and-squared-error\n",
    "\n",
    "\n",
    "    #compute root mean square error\n",
    "    rmse=  math.sqrt(np.mean(np.square(e3)))\n",
    "\n",
    "\n",
    "\n",
    "    return(sse3, rmse, r2) #https://stats.stackexchange.com/questions/184603/in-pca-what-is-the-connection-between-explained-variance-and-squared-error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic diversity (% unique words among total closest 25 words to each atom)\n",
    "\n",
    "def topic_diversity(w2vmodel, dictionary_mat, top_n=25):\n",
    "\n",
    "    topwords=[] #list of list, each innter list includes top N words in that topic\n",
    "\n",
    "    for i in range(0, len(dictionary_mat)): #set to number of total topics\n",
    "        topwords.extend([i[0] for i in w2vmodel.wv.similar_by_vector(dictionary_mat[i],topn=top_n)]) #set for top N words \n",
    "        #print(w2vmodel.wv.similar_by_vector(dictionary[i],topn=N))\n",
    "\n",
    "    uniquewords= set(topwords)\n",
    "    diversity = len(uniquewords)/len(topwords)\n",
    "    return(diversity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### determine the hyperparameter: number of atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i in range(50,500,10):\n",
    "    dictionary, alpha = do_aksvd(model, i, 5, save=False)\n",
    "    td = topic_diversity(model, dictionary, top_n=25)\n",
    "    sse3, rmse, r2 = reconst_qual(model, dictionary, alpha)\n",
    "    res[i] = (td,r2)\n",
    "    print(i,res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(res.keys())\n",
    "y = [res[i][0]*res[i][1] for i in res]\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, alpha = do_aksvd(model, 210, 5, save=True)\n",
    "td = topic_diversity(model, dictionary, top_n=25)\n",
    "_,_,r2 = reconst_qual(model, dictionary, alpha)\n",
    "#td*r2=0.5825234592261156"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
